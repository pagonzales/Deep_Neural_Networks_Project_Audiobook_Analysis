{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e5ff22-eaf7-4993-b047-e0802e2d479c",
   "metadata": {},
   "source": [
    "# Deep Neural Networks Analysis of Audiobooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c00ffb-746d-42c4-992f-06998ccf0073",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e936754-d5a0-446e-bcfa-1d6981c9209f",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f091ca58-9726-4269-8ce0-e2567f0bd7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b276d139-5f9a-4f40-813e-8b28504fad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "raw_csv_data = np.loadtxt('Audiobooks_data.csv', delimiter = ',')\n",
    "unscaled_inputs_all = raw_csv_data[:, 1:-1]\n",
    "targets_all = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbd1678-03ed-43aa-9b36-603c154539e4",
   "metadata": {},
   "source": [
    "### Balancing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6bade4a-6ac9-4144-abe7-adf0db5c36a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many targets are 1 (meaning that the customer did convert)\n",
    "num_one_targets = int(np.sum(targets_all))\n",
    "# Set a counter for targets that are 0 (meaning that the customer did not convert)\n",
    "zero_targets_counter = 0\n",
    "# We want to create a \"balanced\" dataset, so we will have to remove some input/target pairs.\n",
    "# Declare a variable that will do that:\n",
    "indices_to_remove = []\n",
    "# Count the number of targets that are 0. \n",
    "# Once there are as many 0s as 1s, mark entries where the target is 0.\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "# Create two new variables, one that will contain the inputs, and one that will contain the targets.\n",
    "# We delete all indices that we marked \"to remove\" in the loop above.\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis = 0)\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e441c43a-99e7-4622-b445-344dccdb5718",
   "metadata": {},
   "source": [
    "### Standardize the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcfec2df-158b-4939-b154-f9983d440e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a5568-a04f-421c-9c6c-3b9967aa028e",
   "metadata": {},
   "source": [
    "### Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb5656a-eda9-489f-bf2d-64423f48dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the data was collected it might be arranged by date\n",
    "# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.\n",
    "# Since we will be batching, we want the data to be as randomly spread out as possible\n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "# Use the shuffled indices to shuffle the inputs and targets.\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2662ca7-2705-4c83-ac1d-b5149556479f",
   "metadata": {},
   "source": [
    "### Split the data into train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e87e583-18d8-45c0-a0d6-2c98c9af5181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1790.0 3579 0.5001397038278849\n",
      "231.0 447 0.5167785234899329\n",
      "216.0 448 0.48214285714285715\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of samples\n",
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "# Count the samples in each subset, assuming we want 80-10-10 distribution of training, validation, and test.\n",
    "# Naturally, the numbers are integers.\n",
    "train_samples_count = int(0.8 * samples_count)\n",
    "validation_samples_count = int(0.1 * samples_count)\n",
    "\n",
    "# The 'test' dataset contains all remaining data.\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "# Create variables that record the inputs and targets for training\n",
    "# In our shuffled dataset, they are the first \"train_samples_count\" observations\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for validation.\n",
    "# They are the next \"validation_samples_count\" observations, folllowing the \"train_samples_count\" we already assigned\n",
    "validation_inputs = shuffled_inputs[train_samples_count : validation_samples_count + train_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count : validation_samples_count + train_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for test.\n",
    "# They are everything that is remaining.\n",
    "test_inputs = shuffled_inputs[validation_samples_count + train_samples_count:]\n",
    "test_targets = shuffled_targets[validation_samples_count + train_samples_count:]\n",
    "\n",
    "# Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test.\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8124c38-d63e-49f9-ba4a-1065583b1dc3",
   "metadata": {},
   "source": [
    "### Saving the three data sets in .npz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14af055e-a3ee-4ce8-b13f-143786d408dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the three datasets in *.npz.\n",
    "np.savez('Audiobooks_data_train', inputs = train_inputs, targets = train_targets)\n",
    "np.savez('Audiobooks_data_validation', inputs = validation_inputs, targets = validation_targets)\n",
    "np.savez('Audiobooks_data_test', inputs = test_inputs, targets = test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a0b2ad-127a-41a2-ad59-58fff4b2734d",
   "metadata": {},
   "source": [
    "## Create Machine Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c39488e-493b-40c3-861c-daef4e30ada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is not necessary since this is just 1 notebook. \n",
    "#But in case we run this step, this might be useful. Also, so that we know the libraries we will use here\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995e9518-d1d1-4cb7-bcf6-0a33e9f942eb",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7402dca3-bdf1-4cda-8e6c-ec71fa8a612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a temporary variable npz, where we will store each of the three Audiobooks datasets\n",
    "npz = np.load('Audiobooks_data_train.npz')\n",
    "\n",
    "# we extract the inputs using the keyword under which we saved them\n",
    "# to ensure that they are all floats, let's also take care of that\n",
    "train_inputs = npz['inputs'].astype(np.float64)\n",
    "# targets must be int because of sparse_categorical_crossentropy (we want to be able to smoothly one-hot encode them)\n",
    "train_targets = npz['targets'].astype(np.int64)\n",
    "\n",
    "# we load the validation data in the temporary variable\n",
    "npz = np.load('Audiobooks_data_validation.npz')\n",
    "validation_inputs = npz['inputs'].astype(np.float64)\n",
    "validation_targets = npz['targets'].astype(np.int64)\n",
    "\n",
    "# we load the test data in the temporary variable\n",
    "npz = np.load('Audiobooks_data_test.npz')\n",
    "test_inputs = npz['inputs'].astype(np.float64)\n",
    "test_targets = npz['targets'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46710be3-e5f9-440b-8c62-f590af50342f",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "Outline, optimizers, loss, early stopping and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9bc04e4-0378-4800-9455-85de75f54b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 - 2s - 67ms/step - accuracy: 0.4498 - loss: 3.2940 - val_accuracy: 0.6868 - val_loss: 2.3546\n",
      "Epoch 2/100\n",
      "36/36 - 0s - 9ms/step - accuracy: 0.6935 - loss: 1.3093 - val_accuracy: 0.7248 - val_loss: 0.6293\n",
      "Epoch 3/100\n",
      "36/36 - 0s - 13ms/step - accuracy: 0.7368 - loss: 0.5472 - val_accuracy: 0.7852 - val_loss: 0.4799\n",
      "Epoch 4/100\n",
      "36/36 - 0s - 8ms/step - accuracy: 0.7703 - loss: 0.4735 - val_accuracy: 0.7897 - val_loss: 0.4302\n",
      "Epoch 5/100\n",
      "36/36 - 0s - 11ms/step - accuracy: 0.7762 - loss: 0.4377 - val_accuracy: 0.7830 - val_loss: 0.4012\n",
      "Epoch 6/100\n",
      "36/36 - 0s - 8ms/step - accuracy: 0.7770 - loss: 0.4151 - val_accuracy: 0.8098 - val_loss: 0.3832\n",
      "Epoch 7/100\n",
      "36/36 - 0s - 8ms/step - accuracy: 0.7935 - loss: 0.3982 - val_accuracy: 0.8389 - val_loss: 0.3688\n",
      "Epoch 8/100\n",
      "36/36 - 0s - 8ms/step - accuracy: 0.7927 - loss: 0.3887 - val_accuracy: 0.8233 - val_loss: 0.3597\n",
      "Epoch 9/100\n",
      "36/36 - 0s - 8ms/step - accuracy: 0.7997 - loss: 0.3789 - val_accuracy: 0.8255 - val_loss: 0.3558\n",
      "Epoch 10/100\n",
      "36/36 - 0s - 9ms/step - accuracy: 0.7958 - loss: 0.3750 - val_accuracy: 0.8054 - val_loss: 0.3489\n",
      "Epoch 11/100\n",
      "36/36 - 0s - 13ms/step - accuracy: 0.7994 - loss: 0.3754 - val_accuracy: 0.8345 - val_loss: 0.3417\n",
      "Epoch 12/100\n",
      "36/36 - 0s - 8ms/step - accuracy: 0.7946 - loss: 0.3657 - val_accuracy: 0.8389 - val_loss: 0.3370\n",
      "Epoch 13/100\n",
      "36/36 - 0s - 8ms/step - accuracy: 0.8030 - loss: 0.3613 - val_accuracy: 0.8389 - val_loss: 0.3359\n",
      "Epoch 14/100\n",
      "36/36 - 0s - 13ms/step - accuracy: 0.8008 - loss: 0.3610 - val_accuracy: 0.8277 - val_loss: 0.3311\n",
      "Epoch 15/100\n",
      "36/36 - 0s - 9ms/step - accuracy: 0.8066 - loss: 0.3569 - val_accuracy: 0.8300 - val_loss: 0.3302\n",
      "Epoch 16/100\n",
      "36/36 - 0s - 8ms/step - accuracy: 0.8072 - loss: 0.3540 - val_accuracy: 0.8322 - val_loss: 0.3281\n",
      "Epoch 17/100\n",
      "36/36 - 0s - 9ms/step - accuracy: 0.8094 - loss: 0.3519 - val_accuracy: 0.8345 - val_loss: 0.3221\n",
      "Epoch 18/100\n",
      "36/36 - 1s - 20ms/step - accuracy: 0.8058 - loss: 0.3507 - val_accuracy: 0.8412 - val_loss: 0.3215\n",
      "Epoch 19/100\n",
      "36/36 - 0s - 8ms/step - accuracy: 0.8100 - loss: 0.3467 - val_accuracy: 0.8434 - val_loss: 0.3233\n",
      "Epoch 20/100\n",
      "36/36 - 0s - 8ms/step - accuracy: 0.8117 - loss: 0.3472 - val_accuracy: 0.8345 - val_loss: 0.3165\n",
      "Epoch 21/100\n",
      "36/36 - 0s - 12ms/step - accuracy: 0.8092 - loss: 0.3437 - val_accuracy: 0.8434 - val_loss: 0.3215\n",
      "Epoch 22/100\n",
      "36/36 - 1s - 16ms/step - accuracy: 0.8220 - loss: 0.3397 - val_accuracy: 0.8345 - val_loss: 0.3209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16ac880baa0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the input and output sizes\n",
    "input_size = 10 #rating 1-10\n",
    "output_size = 2 #1-will buy 0 will not buy again/convert\n",
    "# Use same hidden layer size for both hidden layers\n",
    "hidden_layer_size = 50\n",
    "\n",
    "# define how the model will look like\n",
    "model = tf.keras.Sequential([\n",
    "    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "    # the final layer is no different, we just make sure to activate it with softmax\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation = 'softmax')    \n",
    "])\n",
    "\n",
    "### Choose the optimizer and the loss function\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# set an early stopping mechanism\n",
    "# let's set patience=2, to be a bit tolerant against random validation loss increases\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "### Training\n",
    "# set the batch size\n",
    "batch_size = 100\n",
    "# set a maximum number of training epochs\n",
    "max_epochs = 100\n",
    "# fit the model\n",
    "model.fit(train_inputs,\n",
    "          train_targets,\n",
    "          batch_size = batch_size,\n",
    "          epochs = max_epochs,\n",
    "          callbacks = [early_stopping], # early stopping\n",
    "          validation_data = (validation_inputs, validation_targets),\n",
    "          verbose = 2   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4ff681-578a-4c77-bba4-5a27b6782b45",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "424c8675-5a3e-4471-8585-dd9786df9879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8151 - loss: 0.3323 \n",
      "nTest Loss: 0.33. Test accuracy: 80.36%\n"
     ]
    }
   ],
   "source": [
    "test_loss,test_accuracy = model.evaluate(test_inputs,test_targets)\n",
    "print('nTest Loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba60c585-058b-45ac-a8da-d4717aebc1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-env)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
